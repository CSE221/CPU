\section{Introduction}
\label{sec: introduction}
\yannis{RE-EVALUATE:INTRO DESCRIBES EP QUEREIS AND FASTR EVALUATION BY EXAMPLES, IMMEDIATELY AFTER RELATED WORK TASKS COLUMN DBS.}

Column-oriented database systems, also called column-stores, have attracted a lot of attention in the past few years~\cite{stonebraker2005c,abadi2007materialization,abadi2008column,sidirourgos2008column,abadi2008query,abadi2009column,plattner2009common,abadi2013design}. In column databases, each database table column is stored separately. Attribute values belonging to the same column are stored contiguously, compressed, and densely packed, as opposed to traditional (row-oriented) database systems that store entire records (rows) one after the other~\cite{abadi2008column}. Column databases are designed mainly for analytical applications~\cite{abadi2008query}.

\begin{figure}
\includegraphics[width=0.4\textwidth]{Example2}
\caption{PubMed database schema. Tables are classified into two categories: entity tables (above the dotted line) and relationship tables. Both the \textit{DT} and the \textit{DA} relationship table have two foreign key columns and also other measure attributes, e.g., column \textit{fre} in \textit{DT}.}
\label{figure:pubmed-schema}
\end{figure}
In this paper, we identify a common novel query type, called {\em Entity-Property} query ({\em EP query}), that has wide applications and requires novel data organization and algorithms for efficient processing. Informally, an SQL query is an EP query if it has the following three phases.
\begin{enumerate}
    \item Obtain \textit{selection} results \matthias{CHANGED intersection TO selection. REMOVE THIS COMMENT IF OK.} on a relationship table (will be defined later) for given property values.
    \item Get \textit{join} results between the selection results and other relationship tables.
    \item \textit{Aggregate} the final join results.
\end{enumerate}

Consider the medical database shown in Figure~\ref{figure:pubmed-schema}. It is a relational representation of a part of PubMed\footnote{\url{http://www.ncbi.nlm.nih.gov/pubmed}}, a medical database that is heavily used by biomedical researchers. The tables are classified into two categories: {\em relationship tables} and {\em entity tables}. Each tuple in an entity table refers to an real-life entity. For example, The \texttt{Terms} entity table has one tuple for each term of the PubMed MeSH ontology. Relationship tables capture the relationship between entities. For instance, the relationship \texttt{DT} has a foreign key \texttt{doc} referring to \texttt{Documents}, a foreign key \texttt{term} referring to \texttt{Terms} and a measure attribute \texttt{fre} recording the number of occurrences of each term in appeared in a document.

\yannis{THE MOTIVATION DESCRIBED A QUERY WITH AGGREGATION}
\matthias{ISN'T COUNT(*) AN AGGREGATION?}

Assume a researcher wants to find out ``how many publications of each author involve the terms \textit{digestive system} and \textit{cancer}''.  Let  $t_1$ and $t_2$ be the IDs of the terms ``digestive system'' and ``cancer'', respectively. To answer this query, the following SQL query is issued:

\begin{tabbing}
\=\texttt{SELECT d.author, count(*)}\\
\>\texttt{FROM DA d}\\
\>\texttt{WHERE }\=\texttt{EXISTS (SELECT * FROM DT dc1}\\
\> \>\texttt{WHERE dc1.term=}$t_1$\texttt{ AND d.doc=dc1.doc)}\\
\>\texttt{AND }\=\texttt{EXISTS (SELECT * FROM DT dc2}\\
\> \>\texttt{WHERE dc2.term=}$t_2$\texttt{ AND d.doc=dc2.doc)}\\
\>\texttt{GROUP BY d.author}
\end{tabbing}

It is an EP query (the detailed definition of EP queries is presented in Section \ref{sec:queries}). To answer this query, the following steps are required for column databases:
\begin{enumerate}
\item Find a set of positions/row IDs $P_1$, $P_2$ for $t_1$ and $t_2$, respectively.
\item Get all the documents $D_1$ and $D_2$ containing terms $t_1$ and $t_2$ using the positions, respectively.
\item Obtain the intersection $I = D_1 \cap D_2$.
\item Join $I$ with $DA$ to find a set of authors $A_i$ for each document $d_i\in I$.
\item Scan each $A_i$ to achieve the aggregated result, i.e., the number of occurrences of each author.
\end{enumerate}
Column databases exhibit the following limitations when answering EP queries.
\begin{itemize}
    \item Large amount of temporary data. E.g., in step 2, a temporary array is created to store all the documents of $t_1$.
    \item Large intermediate results. E.g., the algorithm maintains intersection results in step 3.  
\end{itemize}    
Though lots of optimizations have been studied in literature to speed up performance in column databases, these two limitations could not be overcome.

To overcome these limitations, one observation is that, if all the documents containing a certain term are clustered together off-line, then steps 1 and 2 can be omitted, avoiding temporary data. In addition, if the documents for each term are sorted, then a sort merge algorithm can be applied to generate the intersection results in a pipelined way, avoiding intermediate results. Therefore, we propose a new data organization, which clusters all the related attributes together (called fragments) for each entity. \matthias{REPLACE WITH: which clusters all entities together with entities they are in relation with, along with relation-specific measure attributes.} We observed that a fragment is the perfect storage granularity as opposed to columns (Section~\ref{sec:data-structures}).

In addition, we present a fully pipelined algorithm, FastR, designed based on the new data organization. No random access is required within a fragment, while random access is mandatory for column databases, allowing for additional encoding methods that do not support random access.

\matthias{TODO: explain how column DBs require random access.}

\yannis{BUT HOW DO WE IMPROVE IT? WHAT DEFICIENCY DO YOU POINT OUT TO?}
%
%To overcome there shortcomings, a fully piple algorithm FastR is proposed in this paper. FastR uses a new data structure.
%
%Therefore, we propose a new kind of data organization that avoids maintaining intermediate results and improves join processing. Intuitively, the FastR data organization captures the relevant properties for each entity into \textit{fragments}, which is a smaller storage granularity compared with columns. In addition, the fragments of each entity can be encoded separately, which provides more opportunities for compressions and join operations. For example, if two fragments of different terms are encoded by Run Length Encoding (RLE), then join of these two fragments can be done directly upon the encoded data without decoding.
%
%\yannis{NOT SURE IF THIS GIVES ENOUGH INTUITION. YET, INTRO MAY BECOME TOO BIG WITH COMPLETE INTUITION$+$ FIGURE OF DATA ORGANIZATION}
%In addition, we propose a new algorithm, FastR, to efficiently answer EP queries. The main idea is to calculate the context without generating intermediate results in each iteration by using the data organization. In addition, FastR can fully pipeline the query processing.


\yannis{SUPPORTING THIS STATEMENT REQUIRES A GENERALIZATION OF THE QUERY INTO ENTITIES AND PROPERTIES}

\yannis{COMPARE WITH MONETDB, POSTGRES, THEN EXPLAIN WHY OUR OWN MEMORY COLUMN DB AND COMPARE.}

The following list gives an overview of the main contributions of this paper.
\matthias{Remove references to chapters here? They're already mentioned in the next paragraph.}
\begin{itemize}
\item We present a new kind of queries with many real-life applications (Entity-Property Select-Join-Aggregate SQL queries, EP queries). Current column databases cannot answer this kind of queries efficiently (Section~\ref{sec:queries}).

\item We introduce a new data organization, collecting relevant properties together for each entity \matthias{same as before: shouldn't it be ``group entities together with entities'' or so?}. The storage granularity of this data organization turned out to be better than column-wise storage. In addition, it allows for a wider choice of compression algorithms that do not support random access.

\item We designed a fully optimized main memory database as a baseline to provide a fair comparison (Section~\ref{sec:main-memory}), since existing column databases spend extra time on maintaining databases, e.g., writing logs.

\matthias{delete the following sentence?}
 avoids maintaining intermediate results; (ii) enables wide choice of compressions; and (iii) increases the opportunity for parallel computing (Section \ref{sec:data-structures}).

\item FastR is our implementation of a fully pipelined algorithm, implemented based on the new data organization presented in Section~\ref{sec:algorithm}, avoiding both temporary structures and intermediate results. \matthias{Is the implementation itself really a contribution or just a way to prove that our approach works? Maybe merge with second point?}

\item Our theoretical study of different compression algorithms (Section~\ref{sec:analysis}) shows that if properties satisfy certain conditions then certain compression algorithms always beat the uncompressed version.

\item Finally, a we present a comprehensive set of experiments on the PubMed data set to demonstrate the efficiency of our algorithm. Our results show that our algorithm is up to two orders of magnitude more efficient than MoentDB \matthias{Did not compare with MonetDB, or did we?} and one order of magnitude faster than a fully optimized main-memory column database, while saving space at the same time.
\end{itemize}

The remainder of this paper is organized as follows. In Section~\ref{sec:related_work}, we review the related work. We introduce the schema and EP queries in Section~\ref{sec:schema} and Section~\ref{sec:queries}, respectively. In Section~\ref{sec:main-memory}, we illustrate how main-memory column databases answer EP queries. Section~\ref{sec:data-structures} presents our new data organization, and Section~\ref{sec:algorithm} describes our more efficient algorithm. In Section~\ref{sec:analysis}, we provide a theoretical study of compression algorithms. Finally, we present performance measurements in Section~\ref{experiments}.
